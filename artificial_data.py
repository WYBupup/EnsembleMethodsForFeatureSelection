import numpy as np

"""
generate analysis data
generate(n_samples, n_features, n_significant_features, significant_feature_distribution,
        insignificant_feature_distribution, noise_distribution, labels=linear_labeling)
"""


def linear_labeling(weights=None):
    def labeling(samples, weights=weights):
        """
        operate the labeling
        return sign(weights(D,)@samples(D,N)-mean) if weights is none weights is generated by [-1,1] uniformly distribution
        """
        if weights is None:
            weights = np.random.uniform(-1, 1, samples.shape[0])
        labels = weights.dot(samples)
        labels = np.sign(labels - np.median(labels))
        labels = labels.astype(np.int32)
        return labels

    return labeling


def linear_power_labeling():
    def labeling(samples):
        """
        power the samples(samples**2) and compute weight@samples where weight is generated by uniform distribution
        """
        powers = np.full(samples.shape[0], 2)
        samples_power = np.power(samples.T, powers).T
        return linear_labeling()(samples_power)

    return labeling


def generate(n_samples, n_features, n_significant_features, feature_distribution, insignificant_feature_distribution,
             noise_distribution=None, labeling=linear_labeling()
             ):
    """
    :param n_samples:
    :param n_features:
    :param n_significant_features:
    :param feature_distribution: callable
    :param insignificant_feature_distribution:  callable
    :param noise_distribution: callable
    :param labeling: labeling function callable
    generate artificial data set sample(n_features,n_samples) where
    sample[:n_significant_features]~feature distribution
    sample[n_significant_features]~insignificant feature distribution
    sample is also add with noise ~ noise_distribution
    and the samples is also labeled by labeling function with respect to significant features
    return the samples(D,N)  labels(N,)  feature_labels(D,)
    """
    if n_significant_features <= 0 or n_significant_features > n_samples:
        raise ValueError("significant needs to be positive and inferior to the number of samples")

    if n_significant_features < 1:
        n_significant_features = np.ceil(n_significant_features * n_features)

    samples = np.vstack((
        feature_distribution((n_significant_features, n_samples)),
        insignificant_feature_distribution((n_features - n_significant_features, n_samples))
    ))

    if noise_distribution:
        samples += noise_distribution((n_features, n_samples))

    labels = labeling(samples[:n_significant_features])

    feature_labels = np.hstack((np.ones(int(n_significant_features), dtype=np.int32),
                                -np.ones(int(n_features - n_significant_features), dtype=np.int32)))
    return samples, labels, feature_labels


# following part's s is (n_features,n_samples)
def constant(c):
    return lambda s: np.full(s, c)


def uniform(a, b):
    return lambda s: np.random.uniform(a, b, s)


def laplace(mean, variance):
    return lambda s: np.random.laplace(mean, variance, s)


def normal(mean, variance):
    return lambda s: np.random.normal(mean, variance, s)


# cov is return of uniform(a,b)
def multivariate_normal(mean, cov, k=100):
    def distribution(s, mean=mean, cov=cov):
        """
        :param s: (n_features,n_samples)
        :param mean: callable
        :param cov: callable
        generate s[1] samples~(mean(s[0]), cov) #cov is generated as coded below
        return (D,N)
        """
        m_cov = cov((s[0], s[0]))
        # positive semi-definite matrix
        m_cov = k * m_cov.T.dot(m_cov) / s[0]

        return np.random.multivariate_normal(mean(s[0]), m_cov, s[1]).T  # (D,N)

    return distribution


def multiple_distribution(distributions, shares):
    def distribution(s, distributions=distributions, shares=shares):
        """
        :param s: (n_features, n_samples)
        :param distributions: list of distribution functions
        :param shares:  weights represent the rate of n_features that is generate by corresponding distribution
        generate samples (D,N) where the different part of features are generated by different distributions
        features[sum(share[:i]):sum(share[:i])+share[i]]~distributions[i]
        return (D,N)
        """
        shares = np.array(list(shares))
        shares /= shares.sum()
        shares = np.array([int(share * s[0]) for share in shares])
        shares[0] += s[0] - shares.sum()

        samples = []
        for i, share in enumerate(shares):
            samples.append(distributions[i]((share, s[1])))
        return np.vstack(tuple(samples))

    return distribution
